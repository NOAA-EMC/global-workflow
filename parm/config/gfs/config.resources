#! /usr/bin/env bash
# shellcheck disable=SC2034

########## config.resources ##########
# Set resource information for job tasks
# e.g. walltime, node, cores per node, memory etc.
# Note: machine-specific resources should be placed into the appropriate config file:
#       config.resources.${machine}

if (( $# != 1 )); then

    echo "Must specify an input task argument to set resource variables!"
    echo "argument can be any one of the following:"
    echo "stage_ic aerosol_init"
    echo "prep prepsnowobs prepatmiodaobs"
    echo "atmanlinit atmanlvar atmanlfv3inc atmanlfinal"
    echo "atmensanlinit atmensanlletkf atmensanlfv3inc atmensanlfinal"
    echo "snowanl"
    echo "prepobsaero aeroanlinit aeroanlrun aeroanlfinal"
    echo "anal sfcanl analcalc analdiag fcst echgres"
    echo "upp atmos_products"
    echo "tracker genesis genesis_fsu"
    echo "verfozn verfrad vminmon fit2obs metp arch cleanup"
    echo "eobs ediag eomg eupd ecen esfc efcs epos earc"
    echo "init_chem mom6ic oceanice_products"
    echo "waveinit waveprep wavepostsbs wavepostbndpnt wavepostbndpntbll wavepostpnt"
    echo "wavegempak waveawipsbulls waveawipsgridded"
    echo "postsnd awips gempak npoess"
    echo "ocnanalprep prepoceanobs ocnanalbmat ocnanalrun ocnanalecen ocnanalletkf ocnanalchkpt ocnanalpost ocnanalvrfy"
    exit 1

fi

step=$1

echo "BEGIN: config.resources"

case ${machine} in
  "WCOSS2")
              npe_node_max=128
              # shellcheck disable=SC2034
              mem_node_max="500GB"
    ;;
  "HERA")
              npe_node_max=40
              # shellcheck disable=SC2034
              mem_node_max="96GB"
    ;;
  "GAEA")
              npe_node_max=128
              # shellcheck disable=SC2034
              mem_node_max="251GB"
    ;;
  "ORION")
              npe_node_max=40
              # shellcheck disable=SC2034
              mem_node_max="192GB"
    ;;
  "HERCULES")
              npe_node_max=80
              # shellcheck disable=SC2034
              mem_node_max="512GB"
    ;;
  "JET")
    case ${PARTITION_BATCH} in
      "xjet")
              npe_node_max=24
              # shellcheck disable=SC2034
              mem_node_max="61GB"
        ;;
      "vjet")
              npe_node_max=16
              # shellcheck disable=SC2034
              mem_node_max="61GB"
        ;;
      "sjet")
              npe_node_max=16
              # shellcheck disable=SC2034
              mem_node_max="29GB"
        ;;
      "kjet")
              npe_node_max=40
              # shellcheck disable=SC2034
              mem_node_max="88GB"
        ;;
      *)
        echo "FATAL ERROR: Unknown partition ${PARTITION_BATCH} specified for ${machine}"
        exit 3
    esac
    ;;
  "S4")
    case ${PARTITION_BATCH} in
      "s4")   npe_node_max=32
              # shellcheck disable=SC2034
              mem_node_max="168GB"
        ;;
      "ivy")
              npe_node_max=20
              # shellcheck disable=SC2034
              mem_node_max="128GB"
        ;;
      *)
        echo "FATAL ERROR: Unknown partition ${PARTITION_BATCH} specified for ${machine}"
        exit 3
    esac
    ;;
  "AWSPW")
    export PARTITION_BATCH="compute"
    npe_node_max=40
    # TODO Supply a max mem/node value for AWS
    # shellcheck disable=SC2034
    mem_node_max=""
    ;;
  "CONTAINER")
    npe_node_max=1
    # TODO Supply a max mem/node value for a container
    # shellcheck disable=SC2034
    mem_node_max=""
    ;;
  *)
    echo "FATAL ERROR: Unknown machine encountered by ${BASH_SOURCE[0]}"
    exit 2
    ;;
esac

export npe_node_max

case ${step} in
  "prep")
    wtime='00:30:00'
    npe=4
    npe_node=2
    nth=1
    memory="40GB"
    ;;

  "prepsnowobs")
    wtime="00:05:00"
    npe=1
    nth=1
    npe_node=1
    ;;

  "prepatmiodaobs")
    wtime="00:30:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    ;;

  "aerosol_init")
    wtime="00:05:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    NTASKS=${npe}
    memory="6GB"
    ;;

  "waveinit")
    wtime="00:10:00"
    npe=12
    nth=1
    npe_node=$(( npe_node_max / nth ))
    NTASKS=${npe}
    memory="2GB"
    ;;

  "waveprep")
    wtime="00:10:00"
    npe_gdas=5
    npe_gfs=65
    nth_gdas=1
    nth_gfs=1
    npe_node_gdas=$(( npe_node_max / nth_gdas ))
    npe_node_gfs=$(( npe_node_max / nth_gfs ))
    NTASKS_gdas=${npe_gdas}
    NTASKS_gfs=${npe_gfs}
    memory_gdas="100GB"
    memory_gfs="150GB"
    ;;

  "wavepostsbs")
    wtime_gdas="00:20:00"
    wtime_gfs="03:00:00"
    npe=8
    nth=1
    npe_node=$(( npe_node_max / nth ))
    NTASKS=${npe}
    memory_gdas="10GB"
    memory_gfs="10GB"
    ;;

  # The wavepost*pnt* jobs are I/O heavy and do not scale well to large nodes.
  # Limit the number of tasks/node to 40.
  "wavepostbndpnt")
    wtime="03:00:00"
    npe=240
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    if [[ ${npe_node} -gt 40 ]]; then
        npe_node=40
        export is_exclusive=False
    fi
    NTASKS=${npe}
    ;;

  "wavepostbndpntbll")
    wtime="01:00:00"
    npe=448
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    if [[ ${npe_node} -gt 40 ]]; then
        npe_node=40
        export is_exclusive=False
    fi
    NTASKS=${npe}
    ;;

  "wavepostpnt")
    wtime="04:00:00"
    npe=200
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    if [[ ${npe_node} -gt 40 ]]; then
        npe_node=40
        export is_exclusive=False
    fi
    NTASKS=${npe}
    ;;

  "wavegempak")
    wtime="02:00:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    NTASKS=${npe}
    memory="1GB"
    ;;

  "waveawipsbulls")
    wtime="00:20:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    NTASKS=${npe}
    export is_exclusive=True
    ;;

  "waveawipsgridded")
    wtime="02:00:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    NTASKS=${npe}
    memory_gfs="1GB"
    ;;

  "atmanlinit")
    export layout_x=${layout_x_atmanl}
    export layout_y=${layout_y_atmanl}

    export layout_gsib_x=$(( layout_x * 3 ))
    export layout_gsib_y=$(( layout_y * 2 ))

    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="3072M"
    ;;

  "atmanlvar")
    export layout_x=${layout_x_atmanl}
    export layout_y=${layout_y_atmanl}

    wtime="00:30:00"
    npe_gdas=$(( layout_x * layout_y * 6 ))
    npe_gfs=$(( layout_x * layout_y * 6 ))
    nth_gdas=1
    nth_gfs=${nth_gdas}
    npe_node_gdas=$(( npe_node_max / nth_gdas ))
    npe_node_gfs=$(( npe_node_max / nth_gfs ))
    memory="96GB"
    export is_exclusive=True
    ;;

  "atmanlfv3inc")
    export layout_x=${layout_x_atmanl}
    export layout_y=${layout_y_atmanl}

    wtime="00:30:00"
    npe_gdas=$(( layout_x * layout_y * 6 ))
    npe_gfs=$(( layout_x * layout_y * 6 ))
    nth_gdas=1
    nth_gfs=${nth_gdas}
    npe_node_gdas=$(( npe_node_max / nth_gdas ))
    npe_node_gfs=$(( npe_node_max / nth_gfs ))
    memory="96GB"
    export is_exclusive=True
    ;;

  "atmanlfinal")
    wtime="00:30:00"
    npe=${npe_node_max}
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    ;;

  "snowanl")
    # below lines are for creating JEDI YAML
    case ${CASE} in
      "C768")
        layout_x=6
        layout_y=6
        ;;
      "C384")
        layout_x=5
        layout_y=5
        ;;
      "C192" | "C96" | "C48")
        layout_x=1
        layout_y=1
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
    esac

    export layout_x
    export layout_y

    wtime="00:15:00"
    npe=$(( layout_x * layout_y * 6 ))
    nth=1
    npe_node=$(( npe_node_max / nth ))
    ;;

  "prepobsaero")
    wtime="00:30:00"
    npe=1
    nth=1
    npe_node=1
    memory="96GB"
    ;;

  "aeroanlinit")
    # below lines are for creating JEDI YAML
    case ${CASE} in
      "C768")
        layout_x=8
        layout_y=8
        ;;
      "C384")
        layout_x=8
        layout_y=8
        ;;
      "C192" | "C96")
        layout_x=8
        layout_y=8
        ;;
      "C48" )
        # this case is for testing only
        layout_x=1
        layout_y=1
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
    esac

    export layout_x
    export layout_y
    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="3072M"
    ;;

  "aeroanlrun")
    case ${CASE} in
      "C768")
        layout_x=8
        layout_y=8
        ;;
      "C384")
        layout_x=8
        layout_y=8
        ;;
      "C192" | "C96")
        layout_x=8
        layout_y=8
        ;;
      "C48" )
        # this case is for testing only
        layout_x=1
        layout_y=1
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
    esac

    export layout_x
    export layout_y

    wtime="00:30:00"
    npe_gdas=$(( layout_x * layout_y * 6 ))
    npe_gfs=$(( layout_x * layout_y * 6 ))
    nth_gdas=1
    nth_gfs=1
    npe_node_gdas=$(( npe_node_max / nth_gdas ))
    npe_node_gfs=$(( npe_node_max / nth_gfs ))
    export is_exclusive=True
    ;;

  "aeroanlfinal")
    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="3072M"
    ;;

  "ocnanalprep")
    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="24GB"
    ;;

  "prepoceanobs")
    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="48GB"
    ;;

  "ocnanalbmat")
    npes=16
    case ${OCNRES} in
      "025") npes=480;;
      "050")  npes=16;;
      "500")  npes=16;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${OCNRES}"
        exit 4
    esac

    wtime="00:30:00"
    npe=${npes}
    nth=1
    export is_exclusive=True
    npe_node=$(( npe_node_max / nth ))
    ;;

  "ocnanalrun")
    npes=16
    case ${OCNRES} in
      "025")
        npes=480
        memory="96GB"
        ;;
      "050")
        npes=16
        memory="96GB"
        ;;
      "500")
        npes=16
        memory="24GB"
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${OCNRES}"
        exit 4
    esac

    wtime="00:15:00"
    npe=${npes}
    nth=1
    export is_exclusive=True
    npe_node=$(( npe_node_max / nth ))
    ;;

  "ocnanalecen")
    npes=16
    case ${OCNRES} in
      "025")
        npes=40
        memory="96GB"
        ;;
      "050")
        npes=16
        memory="96GB"
        ;;
      "500")
        npes=16
        memory="24GB"
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${OCNRES}"
        exit 4
    esac

    wtime="00:10:00"
    npe=${npes}
    nth=1
    export is_exclusive=True
    npe_node=$(( npe_node_max / nth ))
    ;;

  "ocnanalletkf")
    npes=16
    case ${OCNRES} in
      "025")
        npes=480
        memory="96GB"
        ;;
      "050")
        npes=16
        memory="96GB"
        ;;
      "500")
        npes=16
        memory="24GB"
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${OCNRES}"
        exit 4
    esac

    wtime="00:10:00"
    npe=${npes}
    nth=1
    export is_exclusive=True
    npe_node=$(( npe_node_max / nth ))
    ;;


  "ocnanalchkpt")
    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    case ${OCNRES} in
      "025")
        memory="128GB"
        npes=40;;
      "050")
        memory="32GB"
        npes=16;;
      "500")
        memory="32GB"
        npes=8;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${OCNRES}"
        exit 4
    esac
    npe=${npes}
    ;;

  "ocnanalpost")
    wtime="00:30:00"
    npe=${npe_node_max}
    nth=1
    npe_node=$(( npe_node_max / nth ))
    ;;

  "ocnanalvrfy")
    wtime="00:35:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="24GB"
    ;;

  "anal")
    wtime_gdas="01:20:00"
    wtime_gfs="01:00:00"
    case ${CASE} in
      "C768")
        npe_gdas=780
        npe_gfs=825
        nth=5
        ;;
      "C384")
        npe_gdas=160
        npe_gfs=160
        nth=10
        ;;
      "C192" | "C96" | "C48")
        npe_gdas=84
        npe_gfs=84
        nth=5
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
        ;;
    esac
    npe_node=$(( npe_node_max / nth ))
    export nth_cycle=${nth}
    export npe_node_cycle=$(( npe_node_max / nth_cycle ))
    export is_exclusive=True
    ;;

  "analcalc")
    wtime="00:15:00"
    npe=127
    export ntasks="${npe}"
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export nth_echgres_gdas=4
    export nth_echgres_gfs=12
    export is_exclusive=True
    memory="48GB"
    if [[ "${CASE}" == "C384" || "${CASE}" == "C768" ]]; then
       memory="${mem_node_max}"
    fi
    ;;

  "analdiag")
    wtime="00:15:00"
    npe=96             # Should be at least twice npe
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="48GB"
    ;;

  "sfcanl")
    wtime="00:20:00"
    npe=${ntiles:-6}
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    ;;

  "fcst" | "efcs")
    export is_exclusive=True

    if [[ "${step}" == "fcst" ]]; then
      _RUN_LIST=${RUN:-"gdas gfs"}
    elif [[ "${step}" == "efcs" ]]; then
      _RUN_LIST=${RUN:-"enkfgdas enkfgfs"}
    fi

    # During workflow creation, we need resources for all RUNs and RUN is undefined
    for _RUN in ${_RUN_LIST}; do
      if [[ "${_RUN}" =~ "gfs" ]]; then
        export layout_x=${layout_x_gfs}
        export layout_y=${layout_y_gfs}
        export WRITE_GROUP=${WRITE_GROUP_GFS}
        export WRTTASK_PER_GROUP_PER_THREAD=${WRTTASK_PER_GROUP_PER_THREAD_GFS}
        ntasks_fv3=${ntasks_fv3_gfs}
        ntasks_quilt=${ntasks_quilt_gfs}
        nthreads_fv3=${nthreads_fv3_gfs}
        nthreads_ufs=${nthreads_ufs_gfs}
        # Will not be set if we are skipping the mediator
        nthreads_mediator=${nthreads_mediator_gfs:-}
      elif [[ "${_RUN}" =~ "gdas" ]]; then
        export layout_x=${layout_x_gdas}
        export layout_y=${layout_y_gdas}
        export WRITE_GROUP=${WRITE_GROUP_GDAS}
        export WRTTASK_PER_GROUP_PER_THREAD=${WRTTASK_PER_GROUP_PER_THREAD_GDAS}
        ntasks_fv3=${ntasks_fv3_gdas}
        ntasks_quilt=${ntasks_quilt_gdas}
        nthreads_fv3=${nthreads_fv3_gdas}
        nthreads_ufs=${nthreads_ufs_gdas}
        nthreads_mediator=${nthreads_mediator_gdas:-}
      fi

      # Determine if using ESMF-managed threading or traditional threading
      # If using traditional threading, set them to 1
      if [[ "${USE_ESMF_THREADING:-}" == "YES" ]]; then
        export UFS_THREADS=1
      else  # traditional threading
        export UFS_THREADS=${nthreads_ufs:-1}
        nthreads_fv3=1
        nthreads_mediator=1
        [[ "${DO_WAVE}" == "YES" ]] && nthreads_ww3=1
        [[ "${DO_OCN}" == "YES" ]] && nthreads_mom6=1
        [[ "${DO_ICE}" == "YES" ]] && nthreads_cice6=1
      fi

      if (( ntiles > 6 )); then
        export layout_x_nest=${layout_x_nest:-10}
        export layout_y_nest=${layout_y_nest:-10}
        export npx_nest=${npx_nest:-1441}
        export npy_nest=${npy_nest:-961}
      fi

      # PETS for the atmosphere dycore
      (( FV3PETS = ntasks_fv3 * nthreads_fv3 ))
      echo "FV3 using (nthreads, PETS) = (${nthreads_fv3}, ${FV3PETS})"

      # PETS for quilting
      if [[ "${QUILTING:-}" == ".true." ]]; then
        (( QUILTPETS = ntasks_quilt * nthreads_fv3 ))
        (( WRTTASK_PER_GROUP = WRTTASK_PER_GROUP_PER_THREAD ))
        export WRTTASK_PER_GROUP
      else
        QUILTPETS=0
      fi
      echo "QUILT using (nthreads, PETS) = (${nthreads_fv3}, ${QUILTPETS})"

      # Total PETS for the atmosphere component
      ATMTHREADS=${nthreads_fv3}
      (( ATMPETS = FV3PETS + QUILTPETS ))
      export ATMPETS ATMTHREADS
      echo "FV3ATM using (nthreads, PETS) = (${ATMTHREADS}, ${ATMPETS})"

      # Total PETS for the coupled model (starting w/ the atmosphere)
      NTASKS_TOT=${ATMPETS}

      # The mediator PETS can overlap with other components, usually it lands on the atmosphere tasks.
      # However, it is suggested limiting mediator PETS to 300, as it may cause the slow performance.
      # See https://docs.google.com/document/d/1bKpi-52t5jIfv2tuNHmQkYUe3hkKsiG_DG_s6Mnukog/edit
      # TODO: Update reference when moved to ufs-weather-model RTD
      MEDTHREADS=${nthreads_mediator:-1}
      MEDPETS=${MEDPETS:-${FV3PETS}}
      (( "${MEDPETS}" > 300 )) && MEDPETS=300
      export MEDPETS MEDTHREADS
      echo "MEDIATOR using (threads, PETS) = (${MEDTHREADS}, ${MEDPETS})"

      CHMPETS=0; CHMTHREADS=0
      if [[ "${DO_AERO}" == "YES" ]]; then
        # GOCART shares the same grid and forecast tasks as FV3 (do not add write grid component tasks).
        (( CHMTHREADS = ATMTHREADS ))
        (( CHMPETS = FV3PETS ))
        # Do not add to NTASKS_TOT
        echo "GOCART using (threads, PETS) = (${CHMTHREADS}, ${CHMPETS})"
      fi
      export CHMPETS CHMTHREADS

      WAVPETS=0; WAVTHREADS=0
      if [[ "${DO_WAVE}" == "YES" ]]; then
        (( WAVPETS = ntasks_ww3 * nthreads_ww3 ))
        (( WAVTHREADS = nthreads_ww3 ))
        echo "WW3 using (threads, PETS) = (${WAVTHREADS}, ${WAVPETS})"
        (( NTASKS_TOT = NTASKS_TOT + WAVPETS ))
      fi
      export WAVPETS WAVTHREADS

      OCNPETS=0; OCNTHREADS=0
      if [[ "${DO_OCN}" == "YES" ]]; then
        (( OCNPETS = ntasks_mom6 * nthreads_mom6 ))
        (( OCNTHREADS = nthreads_mom6 ))
        echo "MOM6 using (threads, PETS) = (${OCNTHREADS}, ${OCNPETS})"
        (( NTASKS_TOT = NTASKS_TOT + OCNPETS ))
      fi
      export OCNPETS OCNTHREADS

      ICEPETS=0; ICETHREADS=0
      if [[ "${DO_ICE}" == "YES" ]]; then
        (( ICEPETS = ntasks_cice6 * nthreads_cice6 ))
        (( ICETHREADS = nthreads_cice6 ))
        echo "CICE6 using (threads, PETS) = (${ICETHREADS}, ${ICEPETS})"
        (( NTASKS_TOT = NTASKS_TOT + ICEPETS ))
      fi
      export ICEPETS ICETHREADS

      echo "Total PETS for ${_RUN} = ${NTASKS_TOT}"

      declare -x "npe_${_RUN}"="${NTASKS_TOT}"
      declare -x "nth_${_RUN}"="${UFS_THREADS}"
      declare -x "npe_node_${_RUN}"="${npe_node_max}"

    done

    case "${CASE}" in
      "C48" | "C96" | "C192")
        declare -x "wtime_gdas"="00:20:00"
        declare -x "wtime_enkfgdas"="00:20:00"
        declare -x "wtime_gfs"="03:00:00"
        declare -x "wtime_enkfgfs"="00:20:00"
        ;;
      "C384")
        declare -x "wtime_gdas"="00:30:00"
        declare -x "wtime_enkfgdas"="00:30:00"
        declare -x "wtime_gfs"="06:00:00"
        declare -x "wtime_enkfgfs"="00:30:00"
        ;;
      "C768" | "C1152")
        # Not valid resolutions for ensembles
        declare -x "wtime_gdas"="00:40:00"
        declare -x "wtime_gfs"="06:00:00"
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
        ;;
    esac

    unset _RUN _RUN_LIST
    unset NTASKS_TOT
    ;;

  "oceanice_products")
    wtime="00:15:00"
    npe=1
    npe_node=1
    nth=1
    memory="96GB"
    ;;

  "upp")
    case "${CASE}" in
      "C48" | "C96")
        npe=${CASE:1}
      ;;
      "C192" | "C384" | "C768" )
        npe=120
        memory="${mem_node_max}"
      ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
      ;;
    esac
    npe_node=${npe}

    nth=1

    wtime="00:15:00"
    if (( npe_node > npe_node_max )); then
      npe_node=${npe_node_max}
    fi
    export is_exclusive=True
    ;;

  "atmos_products")
    wtime="00:15:00"
    npe=24
    nth=1
    npe_node="${npe}"
    export is_exclusive=True
    ;;

  "verfozn")
    wtime="00:05:00"
    npe=1
    nth=1
    npe_node=1
    memory="1G"
    ;;

  "verfrad")
    wtime="00:40:00"
    npe=1
    nth=1
    npe_node=1
    memory="5G"
    ;;

  "vminmon")
    wtime="00:05:00"
    npe=1
    nth=1
    npe_node=1
    memory="1G"
    ;;

  "tracker")
    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=1
    memory="4G"
    ;;

  "genesis")
    wtime="00:25:00"
    npe=1
    nth=1
    npe_node=1
    memory="10G"
    ;;

  "genesis_fsu")
    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=1
    memory="10G"
    ;;

  "fit2obs")
    wtime="00:20:00"
    npe=3
    nth=1
    npe_node=1
    memory="20G"
    ;;

  "metp")
    nth=1
    wtime_gdas="03:00:00"
    wtime_gfs="06:00:00"
    npe=4
    npe_node=4
    export is_exclusive=True
    ;;

  "echgres")
    wtime="00:10:00"
    npe=3
    nth=${npe_node_max}
    npe_node=1
    ;;

  "init")
    wtime="00:30:00"
    npe=24
    nth=1
    npe_node=6
    memory="70GB"
    ;;

  "init_chem")
    wtime="00:30:00"
    npe=1
    npe_node=1
    export is_exclusive=True
    ;;

  "mom6ic")
    wtime="00:30:00"
    npe=24
    npe_node=24
    export is_exclusive=True
    ;;

  "arch" | "earc" | "getic")
    wtime="06:00:00"
    npe=1
    npe_node=1
    nth=1
    memory="4096M"
    ;;

  "cleanup")
    wtime="00:15:00"
    npe=1
    npe_node=1
    nth=1
    memory="4096M"
    ;;

  "stage_ic")
    wtime="00:15:00"
    npe=1
    npe_node=1
    nth=1
    export is_exclusive=True
    ;;

  "atmensanlinit")
    export layout_x=${layout_x_atmensanl}
    export layout_y=${layout_y_atmensanl}

    wtime="00:10:00"
    npe=1
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="3072M"
    ;;

  "atmensanlletkf")
    export layout_x=${layout_x_atmensanl}
    export layout_y=${layout_y_atmensanl}

    wtime="00:30:00"
    npe_enkfgdas=$(( layout_x * layout_y * 6 ))
    npe_enkfgfs=$(( layout_x * layout_y * 6 ))
    nth_enkfgdas=1
    nth_enkfgfs=${nth_enkfgdas}
    npe_node_enkfgdas=$(( npe_node_max / nth_enkfgdas ))
    npe_node_enkfgfs=$(( npe_node_max / nth_enkfgfs ))
    memory="96GB"
    export is_exclusive=True
    ;;

  "atmensanlfv3inc")
    export layout_x=${layout_x_atmensanl}
    export layout_y=${layout_y_atmensanl}

    wtime="00:30:00"
    npe_enkfgdas=$(( layout_x * layout_y * 6 ))
    npe_enkfgfs=$(( layout_x * layout_y * 6 ))
    nth_enkfgdas=1
    nth_enkfgfs=${nth_enkfgdas}
    npe_node_enkfgdas=$(( npe_node_max / nth_enkfgdas ))
    npe_node_enkfgfs=$(( npe_node_max / nth_enkfgfs ))
    memory="96GB"
    export is_exclusive=True
    ;;

  "atmensanlfinal")
    wtime="00:30:00"
    npe=${npe_node_max}
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    ;;

  "eobs" | "eomg")
    wtime="00:15:00"
    wtime="00:30:00"
    case ${CASE} in
      "C768")                 npe=200;;
      "C384")                 npe=100;;
      "C192" | "C96" | "C48") npe=40;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
        ;;
    esac
    nth=2
    # NOTE The number of tasks and cores used must be the same for eobs
    # See https://github.com/NOAA-EMC/global-workflow/issues/2092 for details
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    # Unset npe_node if it is not a multiple of npe_node_max
    # to prevent dropping data on the floor.  This should be set int
    # config.resources.{machine} instead.  This will result in an error at
    # experiment setup time if not set in config.resources.{machine}.
    if [[ $(( npe_node_max % npe_node )) != 0 ]]; then
      unset npe_node_max
    fi
    ;;

  "ediag")
    wtime="00:15:00"
    npe=48
    nth=1
    npe_node=$(( npe_node_max / nth ))
    memory="30GB"
    ;;

  "eupd")
    wtime="00:30:00"
    case ${CASE} in
      "C768")
        npe=480
        nth=6
        ;;
      "C384")
        npe=270
        nth=8
        ;;
      "C192" | "C96" | "C48")
        npe=42
        nth=2
        ;;
      *)
        echo "FATAL ERROR: Resources not defined for job ${step} at resolution ${CASE}"
        exit 4
        ;;
    esac
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    ;;

  "ecen")
    wtime="00:10:00"
    npe=80
    nth=4
    if [[ ${CASE} == "C384" || ${CASE} == "C192" || ${CASE} == "C96" || ${CASE} == "C48" ]]; then
      nth=2
    fi
    npe_node=$(( npe_node_max / nth ))
    export nth_cycle=${nth}
    export npe_node_cycle=${npe_node}
    export is_exclusive=True
    ;;

  "esfc")
    wtime="00:15:00"
    npe=80
    nth=1
    npe_node=$(( npe_node_max / nth ))
    nth_cycle=${nth}
    npe_node_cycle=$(( npe_node_max / nth_cycle ))
    ;;

  "epos")
    wtime="00:15:00"
    [[ ${CASE} == "C768" ]] && wtime="00:25:00"
    npe=80
    nth=1
    npe_node=$(( npe_node_max / nth ))
    export is_exclusive=True
    ;;

  "postsnd")
    wtime="02:00:00"
    npe=40
    nth=8
    npe_node=10
    npe=9
    npe_node=1
    postsnd_req_cores=$(( npe_node * nth ))
    if (( postsnd_req_cores > npe_node_max )); then
        npe_node=$(( npe_node_max / nth ))
    fi
    export is_exclusive=True
    ;;

  "awips")
    wtime="03:30:00"
    npe=1
    npe_node=1
    nth=1
    memory="3GB"
    ;;

  "npoess")
    wtime="03:30:00"
    npe=1
    npe_node=1
    nth=1
    memory="3GB"
    ;;

  "gempak")
    wtime="03:00:00"
    npe_gdas=2
    npe_gfs=28
    npe_node_gdas=2
    npe_node_gfs=28
    nth=1
    memory_gdas="4GB"
    memory_gfs="2GB"
    ;;

  "mos_stn_prep")
    wtime="00:10:00"
    npe=3
    npe_node=3
    nth=1
    memory="5GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    ;;

  "mos_grd_prep")
    wtime="00:10:00"
    npe=4
    npe_node=4
    nth=1
    memory="16GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    ;;

  "mos_ext_stn_prep")
    wtime="00:15:00"
    npe=2
    npe_node=2
    nth=1
    memory="5GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    ;;

  "mos_ext_grd_prep")
    wtime="00:10:00"
    npe=7
    npe_node=7
    nth=1
    memory="3GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    ;;

  "mos_stn_fcst")
    wtime="00:10:00"
    npe=5
    npe_node=5
    nth=1
    memory="40GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    ;;

  "mos_grd_fcst")
    wtime="00:10:00"
    npe=7
    npe_node=7
    nth=1
    memory="50GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    ;;

  "mos_ext_stn_fcst")
    wtime="00:20:00"
    npe=3
    npe_node=3
    nth=1
    memory="50GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    export prepost=True
    ;;

  "mos_ext_grd_fcst")
    wtime="00:10:00"
    npe=7
    npe_node=7
    nth=1
    memory="50GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    ;;

  "mos_stn_prdgen")
    wtime="00:10:00"
    npe=1
    npe_node=1
    nth=1
    memory="15GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    export prepost=True
    ;;

  "mos_grd_prdgen")
    wtime="00:40:00"
    npe=72
    npe_node=18
    nth=4
    memory="20GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    export OMP_NUM_THREADS="${nth}"
    ;;

  "mos_ext_stn_prdgen")
    wtime="00:10:00"
    npe=1
    npe_node=1
    nth=1
    memory="15GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    export prepost=True
    ;;

  "mos_ext_grd_prdgen")
    wtime="00:30:00"
    npe=96
    npe_node=6
    nth=16
    memory="30GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    export OMP_NUM_THREADS="${nth}"
    ;;

  "mos_wx_prdgen")
    wtime="00:10:00"
    npe=4
    npe_node=2
    nth=2
    memory="10GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    export OMP_NUM_THREADS="${nth}"
    ;;

  "mos_wx_ext_prdgen")
    wtime="00:10:00"
    npe=4
    npe_node=2
    nth=2
    memory="10GB"
    NTASK="${npe}"
    export PTILE="${npe_node}"
    export OMP_NUM_THREADS="${nth}"
    ;;

  *)
    echo "FATAL ERROR: Invalid job ${step} passed to ${BASH_SOURCE[0]}"
    exit 1
    ;;

esac

# Get machine-specific resources, overriding/extending the above assignments
if [[ -f "${EXPDIR}/config.resources.${machine}" ]]; then
   source "${EXPDIR}/config.resources.${machine}"
fi

# Check for RUN-specific variables and export them
for resource_var in nth npe npe_node NTASKS memory wtime; do
   run_resource_var="${resource_var}_${RUN}"
   if [[ -n "${!run_resource_var+0}" ]]; then
      declare -x "${resource_var}"="${!run_resource_var}"
   elif [[ -n "${!resource_var+0}" ]]; then
      export "${resource_var?}"
   fi
done

echo "END: config.resources"
