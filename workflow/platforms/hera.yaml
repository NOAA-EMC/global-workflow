# This file configures the workflow to run on Theia

platform: !Platform
  <<: *global_platform_common

  # Evaluate: this must be "false" to ensure disk space availability logic
  # is not run unless this file is for the current platform.
  Evaluate: false

  # name: the name of this platform; this must match what the underlying
  # scripts expect.
  name: HERA

  # detect: this is a function that returns true iff the user is on GAEA
  # and false otherwise
  detect: !calc tools.isdir("/scratch1") and tools.isdir("/scratch2")

  # Additional variables to send to Rocoto XML entities or ecflow edits.
  metasched_more: !expand |
    {metasched.defvar(doc.schedvar.exclusive_queue, doc.accounting.exclusive_partition.exclusive_queue)}
    {metasched.defvar(doc.schedvar.shared_queue, doc.accounting.shared_partition.shared_queue)}
    {metasched.defvar(doc.schedvar.service_queue, doc.accounting.service_partition.service_queue)}
    {metasched.defvar(doc.schedvar.cpu_project, doc.accounting.cpu_project)}

  # DMPDIR - location of the global dump data.  This is used by the observation
  # processing scripts, which are not included in the public release.
  DMPDIR: "/scratch1/NCEPDEV/global/glopara/dump"
  NWPROD: "/scratch1/NCEPDEV/global/glopara/svn/verif/global/tags/vsdb/nwprod"
  RTMFIX: "/scratch1/NCEPDEV/global/glopara/crtm/2.2.3/fix_update/"
  BASE_SVN: "/scratch1/NCEPDEV/global/glopara/svn"

  # BASE_GIT - a directory maintained by emc global model developers
  # that contains recent versions of source code and executables for
  # various git repositories. This is used on some platforms to find
  # executables for this workflow.
  BASE_GIT: "/scratch1/NCEPDEV/global/glopara/git/global-workflow/port2hera"

  config_base_extras: !expand |
    export NDATE="$NWPROD/util/exec/ndate"
    export NHOUR="$NWPROD/util/exec/nhour"
    export WGRIB="$NWPROD/util/exec/wgrib"
    export WGRIB2="/apps/wgrib2/2.0.8/intel/18.0.3.222/bin/wgrib2"
    export COPYGB="$NWPROD/util/exec/copygb"
    export COPYGB2="$NWPROD/util/exec/copygb2"
    export GRBINDEX="$NWPROD/util/exec/grbindex"
    export CNVGRIB="$NWPROD/util/exec/cnvgrib"
    export CNVGRIB21_GFS="$NWPROD/util/exec/cnvgrib"
    export POSTGRB2TBL="$HOMEgfs/sorc/gfs_post.fd/parm/params_grib2_tbl_new"

  # CHGRP_RSTPROD_COMMAND - this specifies the command to use to
  # restrict access to NOAA "rstprod" data restriction class.
  # This only used for observation processing, data assimilation, and
  # data assimilation archiving, which are not in the public release.
  CHGRP_RSTPROD_COMMAND: "chgrp rstprod"

  partitions:
    Evaluate: false
    default_shared: !calc doc.platform.partitions.hera
    default_exclusive: !calc doc.platform.partitions.hera
    default_service: !calc doc.platform.partitions.hera_service
    default_bigmem: !calc doc.platform.partitions.hera_bigmem
    hera:
      <<: *global_partition_common
      specification: hera
      # Queues to use for each job type
      shared_queue: hera
      exclusive_queue: hera

      # Details about the scheduler on this cluster.
      scheduler_settings:
        scheduler_name: Slurm
        parallelism_name: HydraIMPI
        node_type: generic
        physical_cores_per_node: 40
        logical_cpus_per_core: 2
        hyperthreading_allowed: true
        indent_text: "  "
        memory_per_node: !calc (96*1024)

      exclusive_accounting_ref:
        queue: batch
        project: !calc doc.accounting.cpu_project
        partition: hera

      shared_accounting_ref:
        queue: batch
        project: !calc doc.accounting.cpu_project
        partition: hera

      # Generate the actual Python objects for the scheduler, parallelism,
      # and nodes:
      scheduler: !calc |
        tools.get_scheduler(scheduler_settings.scheduler_name, scheduler_settings)
      parallelism: !calc |
        tools.get_parallelism(scheduler_settings.parallelism_name, scheduler_settings)
      nodes: !calc |
        tools.node_tool_for(scheduler_settings.node_type, scheduler_settings)

    hera_service:
      <<: *global_partition_common
      specification: service
      service_queue: service
      scheduler_settings:
        scheduler_name: Slurm
        parallelism_name: HydraIMPI
        node_type: generic
        physical_cores_per_node: 1
        logical_cpus_per_core: 1
        hyperthreading_allowed: false
        indent_text: "  "
        memory_per_node: !calc (64*1024)

      service_accounting_ref:
        queue: batch
        project: !calc doc.accounting.cpu_project
        partition: service

      scheduler: !calc |
        tools.get_scheduler(scheduler_settings.scheduler_name, scheduler_settings)
      parallelism: !calc |
        tools.get_parallelism(scheduler_settings.parallelism_name, scheduler_settings)
      nodes: !calc |
        tools.node_tool_for(scheduler_settings.node_type, scheduler_settings)

    hera_bigmem:
      <<: *global_partition_common
      specification: bigmem
      service_queue: bigmem
      scheduler_settings:
        scheduler_name: Slurm
        parallelism_name: HydraIMPI
        node_type: generic
        physical_cores_per_node: 24
        logical_cpus_per_core: 24
        hyperthreading_allowed: false
        indent_text: "  "
        memory_per_node: !calc (64*1024)

      bigmem_accounting_ref:
        queue: batch
        project: !calc doc.accounting.cpu_project
        partition: bigmem

      scheduler: !calc |
        tools.get_scheduler(scheduler_settings.scheduler_name, scheduler_settings)
      parallelism: !calc |
        tools.get_parallelism(scheduler_settings.parallelism_name, scheduler_settings)
      nodes: !calc |
        tools.node_tool_for(scheduler_settings.node_type, scheduler_settings)

  # Path to pan_df, the program used to get Panasas disk usage information:
  pan_df: pan_df

  # Automatically detect the least used scrub area the user can access:
  least_used_temp: !Immediate 
    - !FirstMax
      - do: /scratch1/NCEPDEV/stmp2
        when: !calc ( int(tools.can_write(do)) and tools.panasas_gb(do) )
      - do: /scratch1/NCEPDEV/stmp4
        when: !calc ( int(tools.can_write(do)) and tools.panasas_gb(do) )
      - do: /scratch2/NCEPDEV/stmp1
        when: !calc ( int(tools.can_write(do)) and tools.panasas_gb(do) )
      - do: /scratch2/NCEPDEV/stmp3
        when: !calc ( int(tools.can_write(do)) and tools.panasas_gb(do) )
      - do: !expand "{doc.user_places.PROJECT_DIR}"
        when: !calc tools.panasas_gb(do)/4
        message: "{do}: use project directory as scrub space"

  # long_term_temp - area for storage of data that must be passed
  # between jobs or shared with programs external to this workflow.
  long_term_temp: !expand "{doc.platform.least_used_temp}/{tools.env('USER')}"

  # short_term_temp - area for data that is only needed within one job:
  short_term_temp: !expand "{doc.platform.least_used_temp}/{tools.env('USER')}"

  # EXP_PARENT_DIR - Parent directory  of the expdir (experiment directory)
  EXP_PARENT_DIR: !expand "{doc.user_places.PROJECT_DIR}"
